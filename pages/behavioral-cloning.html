<!DOCTYPE html>

<html lang="en">
    
    <head>
        <title>gderoo</title>
        <meta charset="utf-8" />
        <META HTTP-EQUIV="CACHE-CONTROL" CONTENT="NO-CACHE">
            
        <link rel="stylesheet" type="text/css" href="/theme/css/bootstrap.css">
        <link rel="stylesheet" type="text/css" href="/theme/css/beemuse.css">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="stylesheet" type="text/css" href="/theme/css/base.css">
    </head>
    
    <body>
        
        <section id="navbar">
            <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
                <div class="container">
                    <div class="navbar-header">
                        <a class="navbar-brand" href="/index.html">gderoo</a>
                    </div>
                    <div class="collapse navbar-collapse">
                        <ul class="nav navbar-nav">
                            <li><a href="/pages/menu.html">Home</a></li>
                            <li><a href="/pages/about-me.html">About Me</a></li>
                            <li><a href="/pages/cv.html">CV</a></li>
                        </ul>
                    </div><!--/.nav-collapse -->
                </div>
            </div>
        </section>
        
        <br>
        <section class="content">
            <div class="row">
                
                <div class="col-xs-12 col-md-8  col-md-offset-2 col-lg-6 col-lg-offset-3">
                    
                    <div class="paper">
                        <header>
                            <h2 class="entry-title">Behavioral Cloning</h2>
                            
                        </header>
                        <footer class="post-info">
                            <b>April 2017</b>
                        </footer><!-- /.post-info -->
                        <br />
                        <div class="entry-content">
                            
                            <p>This is an overview of the third project of the Udacity nanodegreee. In this project, we
                            <ul>
                            <li> Use a video game simulator to collect data of good driving behavior </li>
                            <li> Build and train a convolution neural network in Keras that predicts steering angles from images </li>
                            <li> Test that the model successfully drives around track without leaving the road </li>
                            </ul>
                            
                            <video class="center" width="320" height="240" controls autoplay loop>
                                <source src="/media/behavioral-cloning/video.mp4" type="video/mp4">
                                    <source src="movie.ogg" type="video/ogg">
                                        Your browser does not support the video tag.
                                        </video> <br />
                            
                            <p><b>Data acquisition</b></p>
                            
                            <p>Udacity has created a simulation environment in which we can drive a car. When driving, the simulator captures the driving parameters (most notably the steering angle), together with what the driver sees (front, left and right camera).</p>
                            
                            <img class="center" src="/media/behavioral-cloning/parameters.png">
                            <img class="center" src="/media/behavioral-cloning/leftcenterright.png">
                            </br>
                            
                            <p> After an attempt at creating manual datasets, we noticed that the results were usually jerky without a joystick. In the end, we used the dataset provided <a href="https://d17h27t6h515a5.cloudfront.net/topher/2016/December/584f6edd_data/data.zip"> here</a>, which contained 8036 examples, and 3 times more images. </p>
                            
                            <p>In this dataset, we can use the side cameras to provide additional samples, by adding an angle of +/-0.2. Even so, we end up with about 4.5k images at 0, 0.2 and -0.2 degree respectively.
                            
                            <img class="center" src="/media/behavioral-cloning/histo_before.png">
                            </br>

                            <p><b>Preprocessing</b></p>
                            
                            <p>To augment the data set, we did a series of operations:
                            <ul>
                            <li> random shuffle of the dataset</li>
                            <li> flipping the image horizontally in 50% of cases</li>
                            <li> random change the brightness by a factor of 0.5 to 1.2</li>
                            <li> random lateral shift by +/-50px, while correcting the angle</li>
                            <li> random shearing of the image to simulate a modification of the angle of the road.</li>
                            </ul>
                            This last transformation allows us to use even the images with an angle of 0 without overfitting for that angle, and is one of the most impactful.
                            </p>
                            
                            <p>Below are random examples, starting from the original in top left corner:</p>
                            
                            <img class="center" src="/media/behavioral-cloning/augmentation.png">
                            </br>
                               
                            The resulting distribution of angles is much smoother
                            
                            <img class="center" src="/media/behavioral-cloning/histo_after.png">
                            </br>
                            
                            <p><b>Neural network architecture</b></p>
                            
                            <p>For the architecture, we mostly used the architecture from <a href="https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf" target="_blank">NVIDIA</a> shown below.</p>
                            
                            <img class="center" src="/media/behavioral-cloning/nvidia-architecture.png">
                            </br>

                            <p>We performed a series of modification to adapt to our problem:
                            <ul>
                                <li>We adapted it to our input/output dimensions, i.e. (32,32,3) for input and 43 for output</li>
                                <li>To reach the target of 0.93, we also need to increase model complexity. For that, we use the learnings from the <a href="/pages/facial-detection.html">facial keypoint detection</a> exercise. We double the number of convolutions before pooling, i.e. 2 convolutions in a row instead of 1.</li>
                                <li>We also introduced dropout to limit potential overfitting for this architecture</li>
                            </ul>
                            </p>
                            
                            <img class="center" src="/media/traffic-signs/lenet.png">
                            </br>
                            
                            <p><b>Training</b></p>
                            
                            <p>With a batch size of 128 and an augmented training set of 56337 samples, it takes ~100s per EPOCH. So we create a stopping condition, such that the training stop when the validation error reaches 0.96. </p>
                            
                            <p>Accuracies after 42 EPOCHs:
                            <ul>
                                <li>Training: 0.988</li>
                                <li>Validation: 0.966</li>
                                <li>Test: 0.933</li>
                            </ul>
                            </p>
                            
                            <img class="center" src="/media/traffic-signs/accuracy.png">
                                </br>
                            
                            <p><b>Further improvement</b></p>
                            <p><ul>
                                <li>We could have further tightened the hyperparameters and let the model run to achieve a higher accuracy</li>
                                <li>Preprocessing could also include noise, or the introduction of deformation (e.g. some photographs are not taken purely perpendicularly)</li>
                                <li>In its current implementation, histogram equalization for RGB images may also be distorting the color balance, so adding images with different brightness may achieve better results</li>
                            </ul></p>
                            
                            <br />
                            <p><a href="https://github.com/gderoo/self_driving_car/blob/master/Project%202%20-%20Traffic%20Sign%20Classifier/Traffic_Sign_Classifier.ipynb" target="_blank">GitHub repository</a></p>


                            </div>
                        </div>
                    </div>
                </div>
            
            </section><!-- /#content -->
        
        <footer id="slidingfooter" class="body">
            Original design by Andy Reagan, modified by Guillaume De Roo
        </footer>
    </body>
    </script>
</html>
